import streamlit as st
from dotenv import load_dotenv
import os
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain_chroma import Chroma
from openai import AuthenticationError
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from typing import List
from langchain_core.documents import Document
from operator import itemgetter
import json


# -----------------------------------------------------------
# 0. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# -----------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# -----------------------------------------------------------
# 1. ëª¨ë¸/DB ìºì‹± í•¨ìˆ˜
# -----------------------------------------------------------
@st.cache_resource
def initialize_models():
    llm = ChatOpenAI(model="gpt-5-nano", temperature=0, openai_api_key=OPENAI_API_KEY)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)
    embeddings.embed_query("test")  # API í‚¤ ì¸ì¦ í™•ì¸
    return llm, embeddings

@st.cache_resource
def load_vectorstore_and_retrievers(embeddings, k_value=10):
    PERSIST_DIRECTORY = r'C:\Users\Hopedom\Documents\DS5-LangChain\Langchain-RAG\chromadb\pandas_rst'
    try:
        vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)
        if vectorstore._collection.count() < 10:
            st.warning("ğŸš¨ ë²¡í„° ì €ì¥ì†Œ ë¬¸ì„œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. DB êµ¬ì¶•ì„ ë¨¼ì € ì§„í–‰í•´ ì£¼ì„¸ìš”.")
            st.stop()
    except Exception as e:
        st.error(f"âŒ Chroma DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    pandas_retriever = vectorstore.as_retriever(
        search_kwargs={"k": k_value, "filter": {"library": "pandas"}}
    )
    sklearn_retriever = vectorstore.as_retriever(
        search_kwargs={"k": k_value, "filter": {"library": "scikit-learn"}}
    )

    def combine_docs(result: dict) -> List[Document]:
        return result['pandas_docs'] + result['sklearn_docs']

    combined_retriever = RunnableParallel(
        pandas_docs=pandas_retriever,
        sklearn_docs=sklearn_retriever
    ) | RunnableLambda(combine_docs)

    return {
        'Pandas': pandas_retriever,
        'Scikit-learn': sklearn_retriever,
        'ë‘˜ ë‹¤': combined_retriever
    }

# -----------------------------------------------------------
# 2. RAG Chain êµ¬ì„± & í—¬í¼
# -----------------------------------------------------------
def format_docs(docs: List[Document]) -> str:
    formatted_docs = []
    for doc in docs:
        domain = doc.metadata.get('library', 'General')
        source = doc.metadata.get('source', 'N/A')
        formatted_docs.append(f"ì¶œì²˜ ë„ë©”ì¸: {domain}\níŒŒì¼: {source}\në‚´ìš©: {doc.page_content}")
    return "\n\n".join(formatted_docs)

def get_llm_chain(llm):
    SYSTEM_INSTRUCTION = (
        "You are a professional Python code generation and technical documentation assistant. "
        "Your primary goal is to generate accurate, well-structured Korean answers based **ONLY** on the provided context, "
        "while intelligently handling comparison or partial-context situations."
        
        "---"
        
        "**[RULES]**"
        "1. **Context Relevance:** Always prioritize information derived from the given context. "
        "If the context is fully irrelevant to the question, respond ONLY with: "
        "'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (No Relevant Context)'. "
        
        "2. **Partial Context Mode:** If the context partially covers the question, "
        "generate an answer using the available context and clearly state which parts are missing or generalized."
        
        "3. **Partial Comparison Mode:** If the question includes comparison intent "
        "(e.g., contains 'vs', 'difference', 'ë¹„êµ'), "
        "and the context covers only one side (e.g., only Pandas), "
        "you may combine general domain knowledge to complete the comparison. "
        "Always clarify which portions are from the context and which are from general knowledge."
        
        "4. **Categorical Encoding Comparison:** If the question or context mentions 'ì›í•« ì¸ì½”ë”©', 'get_dummies', or 'OneHotEncoder', "
        "provide a detailed comparison among these methods."
        
        "5. **General Answer:** For all other relevant questions, generate a concise, structured, and accurate answer."

        "---"
        "**[ADAPTIVE FORMAT RULES]**"
        "â€¢ Comparison Questions: summary + Markdown table + key takeaways"
        "â€¢ Function/Method Questions: short summary + numbered list + code examples"
        "â€¢ Conceptual Questions: concise explanation (2-3 sentences) + optional example"
        "â€¢ Code-focused Questions: minimal text + runnable code in Markdown"
        "â€¢ General Questions: structured explanation using bold headings + numbered lists"
        "---"
        "**[STRUCTURE & FORMAT]**"
        "A. Explanation: 2-3 sentence Korean summary"
        "B. Comparison Table (if applicable)"
        "C. Structure: bold headings + numbered lists"
        "D. Code Examples: runnable Python blocks"
        "E. Formatting: Markdown, horizontal rules (`---`) for sections"
        "F. Context Attribution: mark general knowledge vs context-derived content"
        "G. Metadata: do not output source domains, file paths, or previous conversation remnants"
        "---"
        "**[EXAMPLE BEHAVIOR]**"
        "â€¢ Question: 'ì›í•« ì¸ì½”ë”©' | Context: Pandas + Scikit-learn â†’ "
        "Provide summary, Pandas `get_dummies` example, Scikit-learn `OneHotEncoder` example, comparison table, "
        "label context vs general knowledge parts."
        "\n\nContext: {context}"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_INSTRUCTION),
        ("human", "{question}")
    ])
    return prompt | llm | StrOutputParser()

# -----------------------------------------------------------
# 2-1. Self-Evaluation & Auto-Rewrite
# -----------------------------------------------------------
def self_evaluate_response(llm, context: str, question: str, answer: str) -> dict:
    eval_prompt = """
    ì•„ë˜ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”. ì •í™•ì„±, ì™„ì „ì„±, ë§¥ë½ ì í•©ì„±ì„ ê³ ë ¤í•´ 0~100 ì ìˆ˜ì™€ ê°„ë‹¨í•œ ì½”ë©˜íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

    ì§ˆë¬¸: {question}
    ë¬¸ë§¥: {context}
    ë‹µë³€: {answer}

    JSON í˜•íƒœ ì¶œë ¥:
    {{ "score": ìˆ«ì, "comment": "í‰ê°€ ì½”ë©˜íŠ¸" }}
    """

    eval_chain = ChatPromptTemplate.from_messages([
        ("system", "You are an expert evaluator for Python documentation and code explanations."),
        ("human", eval_prompt)
    ]) | llm | StrOutputParser()

    result = eval_chain.invoke({"context": context, "question": question, "answer": answer})
    try:
        return json.loads(result)
    except Exception:
        return {"score": None, "comment": result}

def auto_rewrite_response(llm, context: str, question: str, answer: str) -> str:
    rewrite_prompt = """
    ì´ì „ ë‹µë³€ì„ ê°œì„ í•˜ì—¬ ì •í™•í•˜ê³  ì™„ì „í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    ì´ì „ ë‹µë³€: {answer}
    ì§ˆë¬¸: {question}
    ë¬¸ë§¥: {context}
    ìƒˆë¡­ê²Œ ì‘ì„±ëœ ë‹µë³€ë§Œ Markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
    """
    rewrite_chain = ChatPromptTemplate.from_messages([
        ("system", "You are an expert Python documentation assistant."),
        ("human", rewrite_prompt)
    ]) | llm | StrOutputParser()

    return rewrite_chain.invoke({
        "context": context,
        "question": question,
        "answer": answer
    })
    #return rewrite_chain.invoke({"context": context, "question": question})

# -----------------------------------------------------------
# 3. Streamlit ì•±
# -----------------------------------------------------------
st.set_page_config(page_title="DS5 ë©€í‹° ë„ë©”ì¸ RAG ì±—ë´‡", layout="wide")
st.title("ğŸ“š Pandas & Scikit-learn RAG ì±—ë´‡")
st.caption("ğŸ” ì§ˆë¬¸ì— ë”°ë¼ Pandas, Scikit-learn, ë˜ëŠ” ë‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í†µí•© ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°”: ë„ë©”ì¸ ì„ íƒ + ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
with st.sidebar:
    st.header("âš™ï¸ ê²€ìƒ‰ ì˜µì…˜")
    use_pandas = st.checkbox("Pandas", value=True)
    use_sklearn = st.checkbox("Scikit-learn", value=True)
    k_value = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ (k)", min_value=3, max_value=15, value=10)
    auto_rewrite_threshold = st.slider("ìë™ ì¬ì‘ì„± ì„ê³„ì  ì ìˆ˜", min_value=0, max_value=100, value=80)


# ëª¨ë¸ ë° ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
llm, embeddings = initialize_models()
retrievers = load_vectorstore_and_retrievers(embeddings, k_value=k_value)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state: st.session_state.messages = []

# ì„ íƒ ìƒíƒœì— ë”°ë¥¸ ë¦¬íŠ¸ë¦¬ë²„ ë§¤í•‘
retriever_map = {
    (True, False): 'Pandas',
    (False, True): 'Scikit-learn',
    (True, True): 'ë‘˜ ë‹¤',
    (False, False): 'ë‘˜ ë‹¤'
}
selected_domain = retriever_map[(use_pandas, use_sklearn)]
current_retriever = retrievers[selected_domain]
selected_domain_display = {
    'Pandas': 'Pandas',
    'Scikit-learn': 'Scikit-learn',
    'ë‘˜ ë‹¤': 'Pandas + Scikit-learn (í†µí•© ê²€ìƒ‰)'
}[selected_domain]

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # 1) ì‚¬ìš©ì ë©”ì‹œì§€ ì„¸ì…˜ ì €ì¥ ë° ë°”ë¡œ ì¶œë ¥
    user_msg = {"role": "user", "content": prompt_input}
    st.session_state.messages.append(user_msg)

    with st.chat_message("user"):
        st.markdown(prompt_input)

    # 2) ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        domain_header = f"ğŸ” ê²€ìƒ‰ ë„ë©”ì¸: **{selected_domain_display}**\n\n"
        st.markdown(domain_header, unsafe_allow_html=True)
        response_placeholder = st.empty()
        response_placeholder.info("â³ ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”...")

        # 1ë‹¨ê³„: ê²€ìƒ‰
        try:
            retriever_chain = itemgetter("question") | current_retriever
            retrieved_docs = retriever_chain.invoke({"question": prompt_input})
            context_text = format_docs(retrieved_docs)
        except Exception as e:
            error_msg = f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            response_placeholder.error(error_msg)
            st.session_state.messages.append({"role": "assistant", "content": domain_header + error_msg})
            st.stop()
        
        # 2ë‹¨ê³„: ë‹µë³€ ìƒì„±
        llm_chain = get_llm_chain(llm)
        try:
            response_stream = llm_chain.stream({"context": context_text, "question": prompt_input})
            full_response = response_placeholder.write_stream(response_stream)

            # 3ë‹¨ê³„: Self-Evaluation
            eval_result = self_evaluate_response(llm, context_text, prompt_input, full_response)
            score = eval_result.get("score", "N/A")
            comment = eval_result.get("comment", "")
            st.info(f"âœ… Self-Evaluation ì ìˆ˜: {score}\nğŸ’¬ ì½”ë©˜íŠ¸: {comment}")

            # 4ë‹¨ê³„: ìë™ ì¬ì‘ì„±
            if isinstance(score, (int, float)) and score < auto_rewrite_threshold:
                rewritten_response = auto_rewrite_response(llm, context_text, prompt_input, full_response)
                st.warning(f"ğŸ”„ ì ìˆ˜ {score} ë¯¸ë§Œìœ¼ë¡œ ìë™ ì¬ì‘ì„± ìˆ˜í–‰")
                final_response = rewritten_response  # ì¬ì‘ì„±ëœ ë‹µë³€ë§Œ ìµœì¢… í‘œì‹œ
            else:
                final_response = full_response  # ì ìˆ˜ ì´ìƒì´ë©´ ì›ë˜ ë‹µë³€ ìœ ì§€

            # í™”ë©´ í‘œì‹œ ë° ì„¸ì…˜ ì €ì¥
            response_placeholder.markdown(final_response, unsafe_allow_html=True)
            st.session_state.messages.append({"role": "assistant", "content": domain_header + final_response})

        except AuthenticationError:
            msg = "âŒ ë‹µë³€ ìƒì„± ì¤‘ OpenAI API í‚¤ ì¸ì¦ ì‹¤íŒ¨."
            response_placeholder.error(msg)
            st.session_state.messages.append({"role": "assistant", "content": domain_header + msg})
        except Exception as e:
            msg = f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {type(e).__name__} - {e}"
            response_placeholder.error(msg)
            st.session_state.messages.append({"role": "assistant", "content": domain_header + msg})

            
# ----------------------------------------------------